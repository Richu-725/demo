A2 BACKEND & FRONTEND TO-DO LIST (TXT)

=============================
[ ] You can finish once the code runs without error and ALL requirements in the A2 PDF are satisfied.


=============================
Solve the Environment

[x] Verify Python ≥3.10 and OS file permissions for data/cache and data/tmp.
[x] Ensure required libs are installed: pandas, requests, paho-mqtt (plus stdlib only).
[x] If missing, create an isolated venv and install exactly the above. (Not needed; installed into user site.)
[x] After tasks complete, freeze a minimal requirements.txt for backend/frontend.
[x] No environment variables required; configuration lives inside the code.


=============================
Early Readiness Check (env & inputs)

[x] API key wired into code default (edit ``DEFAULT_API_KEY`` if needed).
[x] Default start/end timestamps embedded (``DEFAULT_START`` / ``DEFAULT_END``).
[x] Output directories exist or are creatable: ``data/cache`` and ``data/tmp``.
[ ] MQTT reachable: broker constants/CLI overrides (needs live broker verification).
[x] Metrics selection valid: assignment defaults ``power,emissions``; CLI still enforces allowed values.
[x] Loop delay set and ≥ 60s: defaults enforce minimum.
[x] Once readiness passes, proceed to implementation changes below.


=============================
TASK 1 — Switch to CSV-first cache (no Parquet)

Goal: Materialize integrated dataset to CSV and load from CSV thereafter; do not rebuild cache from the API for streaming.

Backend (a2_backend.py)
[x] Replace Parquet helpers with CSV equivalents:
    - REMOVE: read_parquet, write_parquet, and any .parquet-specific temp/commit code.
    - REPLACE: find_latest_cache() to discover *.csv, or rename to find_latest_csv().
[x] Canonical cache artifact:
    - Single growing file: data/cache/{network.lower()}_metrics.csv
    - Columns (wide table): facility_id, ts_event, power_mw, co2_t, price, demand, name, fuel, state, lat, lon.
[x] Append-only semantics with de-dup:
    - Implement append_csv_cache(df, path, keys=("facility_id","ts_event")), performing stable sort by (facility_id, ts_event) and removing duplicates on write (atomic: write to *.tmp then os.replace).
[x] Keep manifest.csv but point entries to CSV (resource_id like "{network}:metrics").
[x] stream_latest() must exclusively read the CSV (pd.read_csv with parse_dates for ts_event) and must NOT call the API.
[x] attach_facility_meta() interface unchanged; pivot_metrics()/reconcile_units() remain pure transform steps.
[x] Frontend seed loader must also read CSV (see TASK 3 FRONTEND).


=============================
TASK 2 — Continuous backfill to CSV until API “runs out”

Goal: Iterate API windows forward and keep extending the CSV cache until an empty response (or budget exhausted).

Backend (a2_backend.py)
[x] Introduce backfill_until_empty(settings):
    - Determine window start:
        • If CSV exists: next_start = max(ts_event in CSV) + interval
        • Else: next_start = settings.start (must be provided or derived)
    - Loop over fixed windows [start, end) advancing by interval or a daily window (consistent with A2 PDF), obeying Settings.request_budget.
    - For each non-empty batch:
        • pivot_metrics → attach_facility_meta → append_csv_cache (de-dup)
    - Stop when:
        • A window returns no rows for ALL requested metrics/facilities, OR
        • RequestBudget.remaining == 0.
[x] Facilities metadata cache:
    - On first run, fetch discover_facilities(client, settings.network) and persist data/cache/facilities.csv.
    - On later runs, read facilities.csv unless a refresh flag is provided (e.g., A2_REFRESH_FACILITIES=1).
    - Validate required columns for map/events: facility_id, name, fuel, state, lat, lon; drop rows with missing or out-of-bounds coordinates (already handled).
[x] build_cache(settings) now:
    - Produces/extends the single CSV via the same transforms, not window-labelled Parquet.
    - Updates manifest.csv with the CSV path and retrieved_at timestamp.
[x] Remove now-redundant per-window Parquet naming and tmp_dir usages specific to Parquet.


=============================
TASK 3 — Continuous publishing that never stops (no duplicates)

Goal: The publisher emits NEW rows from the CSV in event-time order, throttled (≥0.1s between messages), forever.

Backend (a2_backend.py)
[x] Watermarking to avoid replays:
    - Introduce data/cache/publish_offsets.json with either a global watermark (last ts_event published) OR per-facility if needed.
    - On startup: load watermark(s); when streaming, filter CSV to rows with ts_event > watermark, sorted by ts_event then facility_id.
    - After each successful publish, advance the watermark and atomically persist publish_offsets.json.
[x] Ensure publish rate control remains in publish_events (≥ 0.1s delay already present).
[x] Topic format unchanged: topic_for(FacilityPoint) uses state/fuel/facility_id.
[x] stream_latest(settings, cache_path=None):
    - Read from CSV; apply watermark filter; convert rows to MetricEvent via _event_from_row (expects power_mw/co2_t/price/demand as optional floats).
[x] loop(settings):
    - Forever: stream_latest (publish new CSV rows) → backfill_until_empty (extend CSV) → sleep(settings.loop_delay ≥ 60s).
    - Catch/log exceptions; never exit unless interrupted.


=============================
TASK 3 (Frontend) — Seed from CSV, not Parquet

Frontend (a2_frontend.py)
[x] _latest_cache_path(): discover latest *.csv (or just point to {network}_metrics.csv if single-file strategy).
[x] _load_seed_data(): pd.read_csv with parse_dates=["ts_event"]; coerce numeric columns.
[x] Update docstrings/help text: “CSV cache” instead of “Parquet cache”.
[x] No change to MQTT subscription, map preparation, or event application logic.


=============================
TASK 4 — Code cleanup (remove redundancy)

Backend
[x] Delete Parquet-specific helpers: read_parquet, write_parquet, *.parquet globs, window-labelled filenames.
[x] Prune tmp_dir logic only used for Parquet (keep atomic write for CSV with *.tmp + replace).
[x] Remove any dead CLI branches or logs referencing Parquet; update messages to “CSV cache”.

Frontend
[x] Remove any Parquet references in errors/comments and sidebar text.
[x] Keep only CSV discovery/reads.


=============================
DEV QUALITY (keep it reusable & testable)

[x] Docstrings on all public functions (inputs/outputs/invariants).
[x] Keep transforms pure (no I/O or prints) — reconcile_units, pivot_metrics, attach_facility_meta.
[ ] Smoke tests: (blocked until OE API credentials and MQTT broker are available)
    - python a2_backend.py --mode build   (creates/extends CSV)
    - python a2_backend.py --mode stream  (publishes from CSV only)
    - python a2_backend.py --mode loop    (end-to-end continuous)
[ ] Lightweight local tests for reconcile_units and pivot_metrics (optional, not submitted).
[x] Logging: INFO for high-level phases; DEBUG for HTTP requests and row counts.


=============================
SUBMISSION PREP

[x] Scope: backend (Tasks 1–3) and frontend (Task 4) only; no extra files.
[x] Deliverables: a2_backend.py, a2_frontend.py, requirements.txt, brief README.
[x] README snippet MUST say: how to run; how to tweak built-in defaults; cache path; row counts; covered timespan; how to verify MQTT.
[ ] Evidence to capture: logs/screenshots showing successful fetch, CSV append (with de-dup), and MQTT publish (with ≥0.1s spacing).


=============================
DEFINITION OF DONE (self-check)

[ ] facilities.csv exists with required columns: facility_id, name, fuel, state, lat, lon. (Created on first build.)
[ ] {network}_metrics.csv exists and grows across runs; no duplicate (facility_id, ts_event) pairs. (Verify after running build.)
[ ] stream_latest publishes only NEW rows (verified via publish_offsets.json watermark). (Requires live MQTT test.)
[x] Per-message delay ≥0.1s enforced; loop round delay ≈ settings.loop_delay (default 60s).
[ ] Frontend loads seed from CSV and renders map/summary; receives live MQTT updates. (Streamlit test pending.)
[x] No Parquet code remains; all references and error messages updated to CSV terminology.
[x] requirements.txt is minimal and exact; code runs cleanly from a fresh environment.

Generated: 2025-11-07T12:19:17.915446
